# !/usr/bin/env python3

import os
import random
from threading import Thread
from utils import (
    Template,
    load_pretrained,
    prepare_infer_args,
    get_logits_processor
)
from transformers import TextIteratorStreamer
import numpy as np
import pandas as pd
import streamlit as st
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline


st.set_page_config(
    page_title="Rank List Labeler",
    page_icon='ğŸ“Œ',
    layout="wide"
)

MODEL_CONFIG = {
    'model_name': '/hy-tmp/gpt2-chinese-cluecorpussmall',             # backbone
    'device': 'cuda:0',                                           # ä½¿ç”¨è®¾å¤‡
    'dataset_file': './data/human_labeled/total_dataset.tsv',       # æ ‡æ³¨æ•°æ®é›†çš„å­˜æ”¾æ–‡ä»¶
    'rank_list_len': 4,                                           # æ’åºåˆ—è¡¨çš„é•¿åº¦
    'max_gen_seq_len': 40,                                        # ç”Ÿæˆç­”æ¡ˆæœ€å¤§é•¿åº¦
    'random_prompts': [                                           # éšæœºpromptæ± 
                        'ä»Šå¤©æˆ‘å»äº†',
                        'è¿™éƒ¨ç”µå½±å¾ˆ',
                        'åˆšæ”¶åˆ°è´§ï¼Œæ„Ÿè§‰',
                        'è¿™éƒ¨ç”µå½±å¾ˆ',
                        'è¯´å®è¯ï¼ŒçœŸçš„å¾ˆ',
                        'è¿™æ¬¡è´­ç‰©æ€»çš„æ¥è¯´ä½“éªŒå¾ˆ'
                    ]
}


######################## é¡µé¢é…ç½®åˆå§‹åŒ– ###########################
RANK_COLOR = [
    'red',
    'green',
    'blue',
    'orange',
    'violet'
]

model_args, data_args, finetuning_args, generating_args = prepare_infer_args()
model, tokenizer = load_pretrained(model_args, finetuning_args)

prompt_template = Template(data_args.prompt_template)
source_prefix = data_args.source_prefix if data_args.source_prefix else ""


######################## ä¼šè¯ç¼“å­˜åˆå§‹åŒ– ###########################
if 'model_config' not in st.session_state:
    st.session_state['model_config'] = MODEL_CONFIG

if 'model' not in st.session_state:
    model_name = st.session_state['model_config']['model_name']
    st.session_state['model'] = GPT2LMHeadModel.from_pretrained(model_name)

if 'tokenizer' not in st.session_state:
    model_name = st.session_state['model_config']['model_name']
    st.session_state['tokenizer'] = BertTokenizer.from_pretrained(model_name)

if 'generator' not in st.session_state:
    st.session_state['generator'] = TextGenerationPipeline(
        st.session_state['model'],
        st.session_state['tokenizer'],
        device=MODEL_CONFIG['device']
    )

if 'current_results' not in st.session_state:
    st.session_state['current_results'] = [''] * MODEL_CONFIG['rank_list_len']

if 'current_prompt' not in st.session_state:
    st.session_state['current_prompt'] = 'ä»Šå¤©æ—©æ™¨æˆ‘å»äº†'

def generate_text_streaming():
    """
    æ¨¡å‹ç”Ÿæˆæ–‡å­—ï¼Œä½¿ç”¨æµå¼è¾“å‡ºã€‚
    """
    current_results = []
    for _ in range(MODEL_CONFIG['rank_list_len']):
        res = st.session_state['generator'](
                st.session_state['current_prompt'],
                max_length=MODEL_CONFIG['max_gen_seq_len'],
                do_sample=True
            )

        # ä½¿ç”¨è¿­ä»£å™¨è¿›è¡Œæµå¼è¾“å‡º
        streamer = TextIteratorStreamer(st.session_state['tokenizer'], timeout=60.0, skip_prompt=True, skip_special_tokens=True)
        gen_kwargs = {
            "input_ids": res[0]["input_ids"],
            "logits_processor": get_logits_processor(),
            "streamer": streamer
        }

        thread = Thread(target=st.session_state['model'].generate, kwargs=gen_kwargs)
        thread.start()

        print("Assistant: ", end="", flush=True)
        for new_text in streamer:
            print(new_text, end="", flush=True)
            current_results.append(new_text)
        print()

    st.session_state['current_results'] = current_results


# def generate_text():
#     """
#     æ¨¡å‹ç”Ÿæˆæ–‡å­—ã€‚
#     """
#     current_results = []
#     for _ in range(MODEL_CONFIG['rank_list_len']):
#         res = st.session_state['generator'](
#                 st.session_state['current_prompt'],
#                 max_length=MODEL_CONFIG['max_gen_seq_len'],
#                 do_sample=True
#             )
#         current_results.extend([e['generated_text'] for e in res])
#     st.session_state['current_results'] = current_results


######################### é¡µé¢å®šä¹‰åŒºï¼ˆä¾§è¾¹æ ï¼‰ ########################
st.sidebar.title('ğŸ“Œ Rank List æ ‡æ³¨å¹³å°')
st.sidebar.markdown('''
    ```python
    ç”¨äºç”Ÿæˆæ¨¡å‹ç”Ÿæˆ Rank List çš„æ ‡æ³¨ã€‚
    ```
''')
st.sidebar.markdown('æ ‡æ³¨æ€è·¯å‚è€ƒè‡ª [InstructGPT](https://arxiv.org/pdf/2203.02155.pdf) ã€‚')
st.sidebar.markdown('RLHF æ›´å¤šä»‹ç»ï¼š[æƒ³è®­ç»ƒChatGPTï¼Ÿå¾—...](https://zhuanlan.zhihu.com/p/595579042)')
st.sidebar.header('âš™ï¸ Model Config')
st.sidebar.write('å½“å‰æ ‡æ³¨é…ç½®ï¼ˆå¯åœ¨æºç ä¸­ä¿®æ”¹ï¼‰ï¼š')
st.sidebar.write(st.session_state['model_config'])

label_tab, dataset_tab = st.tabs(['Label', 'Dataset'])


######################### é¡µé¢å®šä¹‰åŒºï¼ˆæ ‡æ³¨é¡µé¢ï¼‰ ########################
with label_tab:
    with st.expander('ğŸ” Setting Prompts', expanded=True):
        random_button = st.button('éšæœº prompt', help='ä»promptæ± ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªpromptï¼Œå¯é€šè¿‡ä¿®æ”¹æºç ä¸­ MODEL_CONFIG["random_prompts"] å‚æ•°æ¥è‡ªå®šä¹‰promptæ± ã€‚')
        if random_button:
            prompt_text = random.choice(MODEL_CONFIG['random_prompts'])
        else:
            prompt_text = st.session_state['current_prompt']
        
        query_txt = st.text_input('prompt: ', prompt_text)
        if query_txt != st.session_state['current_prompt']:
            st.session_state['current_prompt'] = query_txt
            generate_text()

    with st.expander('ğŸ’¡ Generate Results', expanded=True):
        if st.session_state['current_results'][0] == '':
            generate_text()

        columns = st.columns([1] * MODEL_CONFIG['rank_list_len'])
        rank_results = [-1] * MODEL_CONFIG['rank_list_len']
        rank_choices = [-1] + [i + 1 for i in range(MODEL_CONFIG['rank_list_len'])]
        for i, c in enumerate(columns):
            with c:
                choice = st.selectbox(f'å¥å­{i+1}æ’å', rank_choices, help='ä¸ºå½“å‰å¥å­é€‰æ‹©æ’åï¼Œæ’åè¶Šå°ï¼Œå¾—åˆ†è¶Šé«˜ã€‚ï¼ˆ-1ä»£è¡¨å½“å‰å¥å­æš‚æœªè®¾ç½®æ’åï¼‰')
                if choice != -1 and choice in rank_results:
                    st.info(f'å½“å‰æ’å[{choice}]å·²ç»è¢«å¥å­[{rank_results.index(choice)+1}]å ç”¨ï¼Œè¯·å…ˆå°†å ç”¨æ’åçš„å¥å­ç½®ä¸º-1å†ä¸ºå½“å‰å¥å­åˆ†é…è¯¥æ’åã€‚')
                else:
                    rank_results[i] = choice
                color = RANK_COLOR[i] if i < len(RANK_COLOR) else 'white'
                # st.write(color)
                st.markdown(f":{color}[{st.session_state['current_results'][i]}]")

    with st.expander('ğŸ¥‡ Rank Results', expanded=True):
        columns = st.columns([1] * MODEL_CONFIG['rank_list_len'])
        for i, c in enumerate(columns):
            with c:
                st.write(f'Rank {i+1}ï¼š')
                if i + 1 in rank_results:
                    color = RANK_COLOR[rank_results.index(i+1)] if rank_results.index(i+1) < len(RANK_COLOR) else 'white'
                    st.markdown(f":{color}[{st.session_state['current_results'][rank_results.index(i+1)]}]")

    save_button = st.button('å­˜å‚¨å½“å‰æ’åº')
    if save_button:
        dataset_file_name = MODEL_CONFIG['dataset_file'].split('/')[-1]
        dataset_file_path = MODEL_CONFIG['dataset_file'].replace(dataset_file_name, '')
        if not os.path.exists(dataset_file_path):
            os.makedirs(dataset_file_path)

        if -1 in rank_results:
            st.error('è¯·å®Œæˆæ’åºåå†å­˜å‚¨ï¼', icon='ğŸš¨')
            st.stop()

        with open(MODEL_CONFIG['dataset_file'], 'a', encoding='utf8') as f:
            rank_texts = []
            for i in range(len(rank_results)):
                rank_texts.append(st.session_state['current_results'][rank_results.index(i+1)])
            line = '\t'.join(rank_texts)
            f.write(f'{line}\n')
        
        st.success('ä¿å­˜æˆåŠŸï¼Œè¯·æ›´æ¢promptç”Ÿæˆæ–°çš„ç­”æ¡ˆ~', icon="âœ…")


######################### é¡µé¢å®šä¹‰åŒºï¼ˆæ•°æ®é›†é¡µé¢ï¼‰ #######################
with dataset_tab:
    rank_texts_list = []
    with open(MODEL_CONFIG['dataset_file'], 'r', encoding='utf8') as f:
        for i, line in enumerate(f.readlines()):
            texts = line.strip().split('\t')
            if len(texts) != MODEL_CONFIG['rank_list_len']:
                st.warning(f"error line {i+1}: expeted {MODEL_CONFIG['rank_list_len']} sentence, got {len(texts)}, skipped.")
                continue
            rank_texts_list.append(texts)
    df = pd.DataFrame(
        np.array(rank_texts_list),
        columns=([f'rank {i+1}' for i in range(MODEL_CONFIG['rank_list_len'])])
    )
    st.dataframe(df)